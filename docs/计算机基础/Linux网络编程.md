### 文件IO

1.关于linux的api：

![image-20200910231511664](E:\Typora\imgs\image-20200910231511664.png)



c调用printf函数时，输出内容先进入文件结构体的缓冲区的（8192B,减少磁盘IO，提高效率），但是比如QQ聊天的时候是不需要缓冲区的，应该直接发送， 用write调用就没有缓冲区。

printf（"hello"） 调用 10次，会调用10次write将这5个字符先写入缓冲区，然后调用sys_write一次性写入显示设备中。

![image-20200910232213677](E:\Typora\imgs\image-20200910232213677.png)

![image-20200910233127803](E:\Typora\imgs\image-20200910233127803.png)

这样一个问题？A写文件到磁盘， B读文件， 能立马读到么？

不能。因为首先文件内容，先写到A进程自己的缓冲区中，B是看不到的，然后write调用将文件写入内核缓冲区中（注意，什么时候将内核缓冲区的内容刷到磁盘呢？ OS有一个守护进程， 会定时将内核缓冲区的内容flush，“缓输出”的概念）。B发起read调用，此时即使A写的内容没有flush到磁盘，但是由于已经在内核缓冲区了，B还是能读到的。



- 一个进程打开文件的个数   ulimit -a  可以查看默认1024， 可以修改（``ulimit  -n``），但是最大值跟内存有关

  ![image-20200911101928537](E:\Typora\imgs\image-20200911101928537.png)

![image-20200911102745595](E:\Typora\imgs\image-20200911102745595.png)







​	

##### IO多路复用

首先，select， poll, epoll都是同步IO，读写事件就绪后，自己负责读写，这个过程是阻塞的，而异步IO的实现内核自己负责吧数据从内核空间拷贝到用户空间。

三者的区别：

![image-20200910094542498](E:\Typora\imgs\image-20200910094542498.png)

![image-20200910094550248](E:\Typora\imgs\image-20200910094550248.png)

![image-20200910094602495](E:\Typora\imgs\image-20200910094602495.png)

避免了fdset在内核用户空间来回拷贝

```java
表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。

select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善。
```

select:

![image-20200910105346166](E:\Typora\imgs\image-20200910105346166.png)

描述符还有监听描述符和已连接描述符之分。监听描述符是作为客户端连接请求的一个端点。它通常被创建一次，并**存在于服务器的整个生命周期**。已连接描述符是客户端和服务器之间已经建立起来了的连接的一个端点。服务器每次接受连接请求时都会创建一次，它**只存在于服务器为一个客户端服务的过程中**。

也就是accept会去处理监听描述符（listenfd），然后返回一个已连接描述符，然后再把已连接的文件描述符添加到select监听的描述符中。

调用 select 时，会发生以下事情：

1. 从**用户空间拷贝 fd_set到内核空间**；

2. 注册回调函数__pollwait；

3. **遍历所有 fd**，对全部指定设备做一次 poll（这里的 poll 是一个文件操作，它有两个参数，一个是文件 fd 本身， 一个是当设备尚未就绪时调用的回调函数__pollwait，这个函数把设备自己特有的等待队列传给内核，让内核把当前的进程挂载到其中）；

4. 当设备就绪时，设备就会唤醒在自己特有等待队列中的【所有】节点，于是当前进程就获取到了完成的信号。 poll 文件操作返回的是一组标准的掩码，其中的各个位指示当前的不同的就绪状态（全 0 为没有任何事件触发）， 根据 mask 可对 fd_set 赋值；

5. 如果所有设备返回的掩码都没有显示任何的事件触发，就去掉回调函数的函数指针，进入有限时的睡眠状态， 再**恢复和不断做 poll，再作有限时的睡眠，直到其中一个设备有事件触发为止。**

6. 只要有事件触发，系统调用返回，将 **fd_set 从内核空间拷贝到用户空间**，回到用户态，用户就可以对相关的 fd作进一步的读或者写操作了(recvfrom之类的)。

缺点：假设我们的服务器需要支持100万的并发连接，则在_FD_SETSIZE为1024的情况下，则我们至少需要开辟1k个进程才能实现100万的并发连接。除了进程间上下文切换的时间消耗外，从内核/用户空间大量的无脑内存拷贝、数组轮询等，是系统难以承受的。因此，基于select模型的服务器程序，要达到10万级别的并发访问，是一个很难完成的任务。

epoll：

**刚开始在内核空间中创建一颗红黑树，然后一次性把监听的描述符添加到这颗红黑树上。 然后剩下的就是等待，有响应的时候，红黑树就把有响应的文件描述符做成一个数组返回到用户空间让用户自己去处理，中间的都是一些红黑树的插入删除操作了**。

epoll 原理概述

调用 epoll_create 时，做了以下事情：

1. 内核帮我们在 epoll 文件系统里建了个 file结点；

2. 在内核 cache里建了个红黑树用于存储以后 epoll_ctl传来的 socket；

3. 建立一个 **list 链表**，用于存储准备就绪的事件。 调用 epoll_ctl 时，做了以下事情：

   **当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。** 

调用 epoll_wait时，做了以下事情：

观察 list 链表里有没有数据。有数据就返回，没有数据就 sleep，等到 timeout 时间到后即使链表没数 据也返回。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句 柄而已，所以，**epoll_wait 仅需要从内核态 copy 少量的句柄到用户态而已**。

epoll 的提升：

1. 本身没有最大并发连接的限制，仅受系统中进程能打开的最大文件数目限制；

2. 效率提升：只有活跃的 socket 才会主动的去调用 callback 函数；

   

3. 省去不必要的内存拷贝：epoll 通过内核与用户空间 mmap 同一块内存实现。

   ​	每次注册新的事件到epoll句柄中时(在epoll ctI中指定EPOLL CTL ADD) ,会把所有的fd拷贝进内核,而**不是在epoll wait的时候重复拷贝**。epoll保证 了每个fd在整个过程中只会拷贝一次吗？ 调用epoll ctl的时候？存疑

mmap:

mmap将一个文件或者其它对象映射进内存。文件被映射到多个页上，如果文件的大小不是所有页的大小之和，最后一个页不被使用的空间将会清零。munmap执行相反的操作，删除特定地址区域的对象映射。

当使用mmap映射文件到进程后，就可以直接操作这段虚拟地址进行文件的读写等操作，**不必再调用read，write等系统调用**。但需注意，直接对该段内存写时不会写入超过当前文件大小的内容。

采用共享内存通信的一个显而易见的好处是效率高，因为**进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次数据：一次从输入文件到共享内存区，另一次从共享内存区到输出文件**。实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。

通常使用mmap()的三种情况： **提高I/O效率、匿名内存映射、共享内存进程通信** 。

**eg**: 进程A, B同时读一个文件的同一页：

​	进程A和进程B都将该页映射到自己的地址空间, 当进程A第一次访问该页中的数据时, 它生成一个缺页中断. 内核此时读入这一页到内存并更新页表使之指向它.以后, 当进程B访问同一页面而出现缺页中断时, 该页已经在内存, 内核只需要将进程B的页表登记项指向次页即可. 如下图所示:



### TCP

tcp状态图

![image-20200911153633776](E:\Typora\imgs\image-20200911153633776.png)